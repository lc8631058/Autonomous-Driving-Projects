{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import pooling\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten, Dense, Lambda, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('./data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "#         print(line[0])\n",
    "        lines.append(line)\n",
    "#     lines = lines[1:]\n",
    "\n",
    "# path = './data/'\n",
    "# get_None_idx = []\n",
    "# for idx, line in enumerate(lines):\n",
    "#     line[1] = line[1][1:]\n",
    "#     line[2] = line[2][1:]\n",
    "#     if cv2.imread(path + line[0]) is None:\n",
    "#         get_None_idx.append(idx)\n",
    "    \n",
    "#     if cv2.imread(path + line[1]) is None:\n",
    "#         get_None_idx.append(idx)\n",
    "        \n",
    "#     if cv2.imread(path + line[2]) is None:\n",
    "#         get_None_idx.append(idx)\n",
    "    \n",
    "# images = []\n",
    "# measurments = []\n",
    "# for line in lines:\n",
    "#     # Create train_X\n",
    "#     # read in images from center, left and right cameras    \n",
    "#     image_center = cv2.imread(path + line[0]) # load the center_image\n",
    "#     image_left = cv2.imread(path + line[1]) # load the left_image\n",
    "#     image_right = cv2.imread(path + line[2]) # load the right_image\n",
    "    \n",
    "#     # append every image to images list\n",
    "#     images.extend((image_center, image_left, image_right)) \n",
    "    \n",
    "#     # Create train_Y\n",
    "#     steering_center = float(line[3])# steering measurment\n",
    "#     # create adjusted steering measurements for the side camera images\n",
    "#     correction = 0.2 # this is a parameter to tune\n",
    "#     steering_left = steering_center + correction\n",
    "#     steering_right = steering_center - correction\n",
    "    \n",
    "#     measurments.extend((steering_center, steering_left, steering_right))\n",
    "    \n",
    "# images = np.array(images)\n",
    "# measurments = np.array(measurments)\n",
    "\n",
    "# # Data augumentation: flip image in order to let it includes both clockwise and counter-clockwise images\n",
    "# flip_images = []\n",
    "# measurments_flipped = []\n",
    "# for image, measurement in zip(images, measurements):\n",
    "#     flip_images.append(np.fliplr(image))\n",
    "#     measurments_flipped.append(-measurement)\n",
    "# flip_images = np.array(flip_images)\n",
    "# measurments_flipped = np.array(measurments_flipped)\n",
    "\n",
    "# train_X = np.array(np.concatenate([images, flip_images], axis=0))\n",
    "# train_Y = np.array(np.concatenate([measurments, measurments_flipped], axis=0))\n",
    "\n",
    "# # train, validation split\n",
    "# train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(([1,1], [1,1], [1,1]))\n",
    "b = np.array(([1,0], [1,1], [1,1]))\n",
    "c = np.array(([0,1], [1,1], [1,1]))\n",
    "x = []\n",
    "x.extend((a, b, c))\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 2)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [cv2.flip](https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html?highlight=cv2.flip#cv2.flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(samples, batch_size=50, correc=0.2):\n",
    "    \n",
    "    '''\n",
    "    samples: \n",
    "    correc: parameter to control the deviation of left and right steering angles\n",
    "    batch_size: you know what is that\n",
    "    '''\n",
    "    \n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for start_point in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[start_point : start_point + batch_size]\n",
    "            \n",
    "            images = []\n",
    "            angles = []\n",
    "            for sample in batch_samples:\n",
    "                path = './data/'\n",
    "            # Create train_X\n",
    "            # read in images from center, left and right cameras    \n",
    "                image_center = cv2.imread(path + sample[0]) # load the center_image\n",
    "                image_left = cv2.imread(path + sample[1]) # load the left_image\n",
    "                image_right = cv2.imread(path + sample[2]) # load the right_image\n",
    "    \n",
    "            # append every image to images list\n",
    "                images.extend((image_center, image_left, image_right)) \n",
    "    \n",
    "            # Create train_Y\n",
    "                steering_center = float(sample[3])# steering measurment\n",
    "            # create adjusted steering measurements for the side camera images\n",
    "                correction = correc # this is a parameter to tune\n",
    "                steering_left = steering_center + correction\n",
    "                steering_right = steering_center - correction\n",
    "    \n",
    "                angles.extend((steering_center, steering_left, steering_right))\n",
    "    \n",
    "            images = np.array(images)\n",
    "            angles = np.array(angles)\n",
    "#             print(images.shape)\n",
    "            # Data augumentation: flip image in order to let it includes both clockwise and counter-clockwise images\n",
    "            images_flipped = []\n",
    "            angles_flipped = []\n",
    "            for image, angle in zip(images, angles):\n",
    "                images_flipped.append(cv2.flip(image, 1))\n",
    "                angles_flipped.append(angle*-1.0)\n",
    "            images_flipped = np.array(images_flipped)\n",
    "            angles_flipped = np.array(angles_flipped)\n",
    "#             print(images[0].shape)\n",
    "            train_X = np.array(np.concatenate([images, images_flipped], axis=0))\n",
    "#             print(train_X.shape)\n",
    "            train_Y = np.array(np.concatenate([angles, angles_flipped], axis=0))\n",
    "            train_X, train_Y = sklearn.utils.shuffle(train_X, train_Y)\n",
    "        \n",
    "            yield train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start_point in range(0, 350, 32):\n",
    "    batch_samples = lines[start_point : start_point + batch_size]\n",
    "            \n",
    "    images = []\n",
    "    angles = []\n",
    "    for sample in batch_samples:\n",
    "        path = './data/'\n",
    "            # Create train_X\n",
    "            # read in images from center, left and right cameras    \n",
    "        image_center = cv2.imread(path + sample[0]) # load the center_image                \n",
    "        image_left = cv2.imread(path + sample[1]) # load the left_image\n",
    "        image_right = cv2.imread(path + sample[2]) # load the right_image\n",
    "#         print(image_right.shape)\n",
    "        # append every image to images list\n",
    "        images.extend((image_center, image_left, image_right))\n",
    "        \n",
    "        steering_center = float(sample[3])# steering measurment\n",
    "            # create adjusted steering measurements for the side camera images\n",
    "        correction = 0.2 # this is a parameter to tune\n",
    "        steering_left = steering_center + correction\n",
    "        steering_right = steering_center - correction\n",
    "        angles.extend((steering_center, steering_left, steering_right))\n",
    "    \n",
    "    images = np.array(images)\n",
    "    angles = np.array(angles)\n",
    "    print(images.shape)\n",
    "    images_flipped = []\n",
    "    angles_flipped = []\n",
    "    for image, angle in zip(images, angles):\n",
    "        images_flipped.append(cv2.flip(image, 1))\n",
    "        angles_flipped.append(angle*-1.0)\n",
    "    images_flipped = np.array(images_flipped)\n",
    "    angles_flipped = np.array(angles_flipped)\n",
    "#             print(images[0].shape)\n",
    "    train_X = np.concatenate([images, images_flipped], axis=0)\n",
    "#             print(train_X.shape)\n",
    "    train_Y = np.concatenate([angles, angles_flipped], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To build the Network Archtecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation split\n",
    "# actually we split the path but not total data, the data will be read by generator\n",
    "train_samples, validation_samples = train_test_split(lines, test_size=0.2)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "correct = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = data_generator(train_samples, batch_size=batch_size, correc=correct)\n",
    "validation_generator = data_generator(validation_samples, batch_size=batch_size, correc=correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 9600/15742 [=================>............] - ETA: 285s - loss: 0.1079"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-334-37742e2517fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                     \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                     verbose=1)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Behavioral_Cloning_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1552\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1898\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1900\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1901\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h, w, d = 160, 320, 3\n",
    "cropping_upper, cropping_bottom = 50, 20\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(h, w, d)))\n",
    "model.add(Cropping2D(cropping=((cropping_upper,cropping_bottom), (0,0))))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(24, 5,5, subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(36, 5,5, subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(48, 5,5, subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3,3, subsample=(1,1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3,3, subsample=(1,1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1164))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch=\n",
    "                                    len(train_samples), validation_data=\n",
    "                                    validation_generator, nb_val_samples=\n",
    "                                    len(validation_samples), nb_epoch=10,\n",
    "                                    verbose=1)\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "### print the keys contained in the history object \n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
